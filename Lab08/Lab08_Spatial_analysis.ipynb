{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d8531b",
   "metadata": {},
   "source": [
    "# Lab 08: Spatial analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da6f20",
   "metadata": {},
   "source": [
    "Author: **N.J. de Winter** (*n.j.de.winter@vu.nl*)<br>\n",
    "Assitant Professor Vrije Universiteit Amsterdam<br>\n",
    "Statistics and Data Analysis Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92b3df",
   "metadata": {},
   "source": [
    "\n",
    "## Learning goals:\n",
    "\n",
    "* Apply and improve your knowledge of Python and Jupyter\n",
    "* Learn to work with spatial datasets and analyze the characteristics of spatial data.\n",
    "* Learn to apply spatial analysis to create maps of variables based on a limited number of measurements in the area.\n",
    "* Obtain a feeling for the reliability of spatial analysis and the trustworthyness of the resulting maps.\n",
    "* Develop a feeling for how statistical tools can help you, but you still require *your interpretation* to draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a47a00",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this lab, you will get to experiment for the first time with **spatial analysis**. Spatial analysis is a big and important field in data science, and you see the outcome of its application on a daily basis all around you: Think about temperature maps on the weather forecast, elevation maps you use in the field and maps used to plot and assess risk of hazards such as forest fires. Everything that is plotted on a map based on a limited number of datapoints is subject to some kind of spatial interpolation!\n",
    "\n",
    "Needless to say, spatial analysis requires you to **think in multiple dimensions** (at least two), making statistical operations much more complex. Although this lab is just an introduction to make you familiar with spatial analysis techniques, it will relfect some of that complexity. Therefore, expect to be spending more time on this one than you are used to. Don't worry, we will take this step by step and as a result you will learn to produce some nice maps. (It will be worth it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f739db0",
   "metadata": {},
   "source": [
    "**Exercise 1:** The data we need today (`geost_dat.mat`) is again in the `.mat` format. Remember from `Lab05` and `Lab07` how to load a file like this. Load the data and also import the packages `numpy`, `matplotlib.pyplot` and `loadmat` from `scipy.io` (you need the latter for loading the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cbd97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cde146d",
   "metadata": {},
   "source": [
    "## Inspecting the data\n",
    "\n",
    "The data file contains `x` and `y` coordinates, and `z` values in a dictionary. We first have to read the `x`, `y` and `z`-variables from the dictionary. You have seen how to do this in `Lab07`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2abbd",
   "metadata": {},
   "source": [
    "**Exercise 2:** Extract the variables `x`, `y` and `z` from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d62e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cffd06f",
   "metadata": {},
   "source": [
    "Now, let’s plot all the locations for which we have data in a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab027e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0fbe16",
   "metadata": {},
   "source": [
    "## Creating a variogram\n",
    "\n",
    "To analyze our spatial dataset, we want to calculate *distances* and *variances* between different pairs of observations. We will use the function `meshgrid` (`numpy` package) to facilitate this. First, let’s study the `help` of the function `meshgrid`, and then execute the function on the x and y coordinates, and z values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a573761",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.meshgrid) # Call help for the meshgrid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d630851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the meshgrid function, we will create some new variables this way\n",
    "X1, X2 = np.meshgrid(x, x)\n",
    "Y1, Y2 = np.meshgrid(y, y)\n",
    "Z1, Z2 = np.meshgrid(z, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06da092",
   "metadata": {},
   "source": [
    "**Question 1:** Study the output of the meshgrid function you just applied. Explain in your own words what the meshgrid function did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c9a44",
   "metadata": {},
   "source": [
    "**Answer 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e387a411",
   "metadata": {},
   "source": [
    "The distance between all pairs of observations can now be calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30347584",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.sqrt(np.square((X1 - X2)) + np.square(Y1 - Y2)) # Calculate distance between all pairs of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030c5b5",
   "metadata": {},
   "source": [
    "The semivariance between all pairs of observations is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 0.5 * (np.square(Z1 - Z2)) # Calculate semivariance between all pairs of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff159a15",
   "metadata": {},
   "source": [
    "**Question 2:** Inspect the newly created matrices `D` and `G` and make sure you understand what information they contain. What is striking about the symmetry of these matrices?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bba990",
   "metadata": {},
   "source": [
    "**Answer 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6d57d5",
   "metadata": {},
   "source": [
    "We will create a matrix `I` that has ones for the lower triangle (and zeros for the upper right). We can then use this matrix `I` to select only one set (of the 2) of paired observations. To do this, first we create an index matrix (`indx`), which counts from 1 to the total number of observations (`z`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = range(z.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c19183",
   "metadata": {},
   "source": [
    "We then use this indx with meshgrid function to create matrices that duplicate the index vector in the column (`C`) and row (`R`) directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, R = np.meshgrid(indx, indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01654c",
   "metadata": {},
   "source": [
    "Finally, we assign 1 to cells in a new matrix `I` when `R` > `C`, and otherwise the cells get assigned a 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = R > C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29015120",
   "metadata": {},
   "source": [
    "Now we can make a scatter plot of semivariance versus distance for the cells where `I` is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f0af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(D[I], G[I])\n",
    "plt.xlabel('Distance between observations')\n",
    "plt.ylabel('Semivariance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b59ed",
   "metadata": {},
   "source": [
    "This *variogram cloud* is somewhat useful for exploration, however, we need to calculate **mean semivariance** for binned distances if we really want to quantify spatial dependencies. We need to have a suitable **lag** interval to define classes over which we can average semivariances. The *mean minimum distance* between pairs is a good estimate for a lag interval. This *mean minimum distance* is the average of the minimum distance of each observation to its nearest neighbour observation. To calculate this, we need to get rid of the zero diagonal of the distance matrix `D`. We create a copy of our distance matrix, and set the diagonal to `nan` (meaning \"not a number\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5280978",
   "metadata": {},
   "outputs": [],
   "source": [
    "D2 = np.copy(D) # Make a copy of the matrix D\n",
    "i, j = np.indices(D2.shape) # here we get the x and y indices of the matrix\n",
    "D2[i == j] = np.nan # the diagonal is where both indices are identical. We set those values to nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30f089",
   "metadata": {},
   "source": [
    "After this, we can easily calculate the minimum distance for each observation, and the mean minimum distance of all observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa82f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = np.nanmean(np.nanmin(D2, axis = 0)) # Take the mean of all minimum differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1f9a63",
   "metadata": {},
   "source": [
    "**Question 3:** What is the lag value of this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee06f2",
   "metadata": {},
   "source": [
    "**Answer 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e8a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d646e18",
   "metadata": {},
   "source": [
    "Estimated variogram values tend to become more erratic with increasing distances between observations. As a rule of thumb, the *half maximum distance* is a suitable range (maximum value) for variogram analysis. The half maximum distance is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c60220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmd = np.max(D) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05603b71",
   "metadata": {},
   "source": [
    "**Question 4:** What is the value of the half maximum distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c8597",
   "metadata": {},
   "source": [
    "**Answer 4:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ac115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb0382",
   "metadata": {},
   "source": [
    "Since the smallest bin is equal to the lag and the maximum is equal to the half maximum distance, the number of bins can be calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b564f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lags = np.floor(hmd / lag)\n",
    "print(max_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aaeb77",
   "metadata": {},
   "source": [
    "**Exercise 3:** The `max_lags` value is 16. Have a look at what the `floor` function (numpy) does. We used it here because we don't want a decimal number for the number of classes, but a whole integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713385a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb23ea12",
   "metadata": {},
   "source": [
    "Now, we will calculate in which distance bin each pair of observations belongs. Have a look at what the `ceil` function (numpy) does. Again, we want whole integers here because they represent distance class numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387238c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.ceil)\n",
    "LAGS = np.ceil(D / lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88cd97",
   "metadata": {},
   "source": [
    "Now get ready for some Python magic! We will loop, using a *list comprehension*, over all distance classes to calculate the *mean distance* and *mean semivariance* of all observations in each distance bin. *List comprehensions* are shorter and faster ways of returning the result of a *for loop* in a *list*. In this case, the list is the list of bins (`range(1, int(max_lags) + 1)`) and for each instance of the list, the mean value for distance and semivariance is calculated for all observations that belong in that bin (according to the value in `LAGS`).\n",
    "\n",
    "This is the mean distance for each distance bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DE = [np.mean(D[LAGS == maxlag]) for maxlag in range(1, int(max_lags) + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3aa75",
   "metadata": {},
   "source": [
    "Similarly, this is the mean semivariance for each distance bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebd828",
   "metadata": {},
   "outputs": [],
   "source": [
    "GE = [np.mean(G[LAGS == maxlag]) for maxlag in range(1, int(max_lags) + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecfb45",
   "metadata": {},
   "source": [
    "**Exercise 4:** Now, plot the semivariogram using the mean distance (`DE`) on the horizontal axis and the mean semivariance (`GE`) on the vertical axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657e3c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16eaff1d",
   "metadata": {},
   "source": [
    "We will also add the *population variance* in this graph. This is the variance in the data, regardless of spatial dependencies. Let’s check the `help` of the `var` function, and then we will use this function to calculate the variance in the `z` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.var)\n",
    "var_z = np.var(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8735b3",
   "metadata": {},
   "source": [
    "Now, we will draw a horizontal line in the plot that represents the population variance. Let's check the help of the `matplotlib hlines` function and then apply it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e46665",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(plt.hlines)\n",
    "\n",
    "plt.scatter(DE, GE) # Replot the plot from before\n",
    "plt.xlabel('Mean distance between observations')\n",
    "plt.ylabel('Mean semivariance')\n",
    "plt.hlines(var_z, 0, np.max(DE), color = 'red') # Add population variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e969459",
   "metadata": {},
   "source": [
    "To make sure the graph is nicely plotted we will shift the limit of the y axis a bit. For example, the limit of y axis could have a value that is 10 % larger than the highest observed value. Let’s calculate this value, after which we can use it to set the limits of the y axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dece7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yl = 1.1 * np.max(GE)\n",
    "plt.scatter(DE, GE) # Replot the plot from before\n",
    "plt.hlines(var_z, 0, np.max(DE), color = 'red') # Add population variance\n",
    "plt.ylim((0, yl)) # Change the limit of the y axis\n",
    "plt.xlabel('Averaged distance between observations')\n",
    "plt.ylabel('Averaged semivariance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c99264",
   "metadata": {},
   "source": [
    "We can see that the semivariance is small for small distances, and increases for larger distances. The semivariance reaches a plateau that is almost equal to the population variance. This variance is called the **sill**. The distance at which the sill is reached, is called the **range**. This is the length over which spatial dependencies in the data are active. We will plot the variogram again in a new figure, after which we plot some pre-defined regression lines on top of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2449614",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(DE, GE)\n",
    "plt.hlines(var_z, 0, np.max(DE), color = 'red')\n",
    "plt.ylim((0, yl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b62987",
   "metadata": {},
   "source": [
    "## Approximating the shape of the variogram using a model\n",
    "\n",
    "If we want to use the shape and characteristics of our variogram to predict the values of `z` for the areas in between our observations, we need to find a model that approximates the shape of the variogram so we can estimate for each point on a `x`-`y` grid what the `z` value should be and how closely it resembles the neighbouring points. Let's try to do this using a few different models.\n",
    "\n",
    "First, we will try to fit a **spherical model** to the data without a **nugget**. These are the parameters of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4847b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuggetS = 0 # nugget\n",
    "sillS = 0.803 # sill\n",
    "rangeS = 45.9 # range\n",
    "lags = np.arange(0, np.max(DE)) # bin values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b30d1",
   "metadata": {},
   "source": [
    "The **nugget** is the intercept of the curve with the y-axis. This is often 0 for spatial data, but could also be a small value.\n",
    "\n",
    "We can calculate the semivariance for each bin according to this model using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5de2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gsph = nuggetS + (sillS * (1.5 * lags / rangeS - 0.5 * (lags / rangeS) ** 3) * (lags <= rangeS) + sillS * (lags > rangeS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a07ef",
   "metadata": {},
   "source": [
    "Now we can plot the spherical model as a green line on top of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "yl = 1.1 * np.max(GE)\n",
    "plt.scatter(DE, GE) # Replot the plot from before\n",
    "plt.plot(lags, Gsph, color = 'green') # Plot the spherical model\n",
    "plt.hlines(var_z, 0, np.max(DE), color = 'red') # Add population variance\n",
    "plt.ylim((0, yl)) # Change the limit of the y axis\n",
    "plt.xlabel('Averaged distance between observations')\n",
    "plt.ylabel('Averaged semivariance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d5632",
   "metadata": {},
   "source": [
    "Secondly, let's try to fit an **exponential model** to our data. The parameters for this model are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuggetE = 0.0239 # nugget\n",
    "sillE = 0.78 # sill\n",
    "rangeE = 45 # range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607dd3b",
   "metadata": {},
   "source": [
    "We can calculate the modelled semivariance values for this exponental model using the following formula (Note the difference between it and the spherical model!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514afea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gexp = nuggetE + sillE * (1 - np.exp(-3 * lags / rangeE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b61254",
   "metadata": {},
   "source": [
    "**Exercise 5:** Add the exponential model to your plot using a blue line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb278f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cee833e",
   "metadata": {},
   "source": [
    "Finally, we will try to fit a linear model to our data. These are the parameters we will use for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ad4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuggetL = 0.153 # nugget\n",
    "slopeL = 0.0203 # slope of the line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b0f5c",
   "metadata": {},
   "source": [
    "The linear model is the simplest model we use (in fact it is just a linear regression, see `Lab01`). It's semivariance values can be calculated from the mean distances of each bin as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "Glin = nuggetL + slopeL * lags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad282499",
   "metadata": {},
   "source": [
    "**Exercise 6:** Add the linear model to your plot using a purple line and add a legend for all the colors using the following line at the end of your plot statement: `plt.legend(labels = ['Sperical model', 'Exponential model', 'Linear model', 'Variogram', 'Population variance'])`. Make sure the order in which you plot your data and models matches the order in this plot legend statement. You should end up with a plot that shows the data with all three model fits using differently colored lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b063471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b05c8ac",
   "metadata": {},
   "source": [
    "**Question 5:** Which model best approximates the data in the variogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274fcf2",
   "metadata": {},
   "source": [
    "**Answer 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72870d",
   "metadata": {},
   "source": [
    "## Kriging: Interpolating between our spatial observations\n",
    "\n",
    "We will use the exponential model with nugget for **kriging**. We will use the  semivariance value derived from this model as weighting factors for the interpolation of `z` values between observations. To do so, first we will apply the model on all distance values in our `D` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afdacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_mod = (nuggetE + sillE * (1 - np.exp(-3 * D / rangeE))) * (D > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44b580",
   "metadata": {},
   "source": [
    "For the calculations that follow, we need to add a row and a column of ones to the matrix, and we will change the bottom right corner of the matrix to a zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(x) # n is the number of observations.\n",
    "G_mod = np.hstack((G_mod, np.ones((n, 1)))) # This adds a row of ones.\n",
    "G_mod = np.vstack((G_mod, np.ones((1, n + 1)))) # This adds a column of ones (note the n + 1 is needed because we already added the new row).\n",
    "G_mod[n, n] = 0 # This sets the value of the lower right cell at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb6060",
   "metadata": {},
   "source": [
    "`G_mod` now contains modelled semivariance values for each combination of observations. To do kriging, we need the inverse of the semivariance such that pairs with smaller semivariances have a larger weight in the eventual calculation. The `inverse` function (`np.linalg.inv`) will do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef94285",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_inv = np.linalg.inv(G_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c0a90",
   "metadata": {},
   "source": [
    "We will apply the kriging interpolation to a rectangular grid ranging from 0 to 200 in `x` and `y` with a grid size of 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29167477",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.arange(0, 201, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2263360",
   "metadata": {},
   "source": [
    "The `meshgrid` function allows us to easily create matrices whose rows and columns contain all combinations of x and y values just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xg1, Xg2 = np.meshgrid(R, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72172332",
   "metadata": {},
   "source": [
    "However, we do not want `matrices` of these combinations of x and y values, but we need them in `vectors`. In other words, we have to `reshape` the data. Have a look at the `help` for the `reshape` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2e33a",
   "metadata": {},
   "source": [
    "The following function will reshape the `Xg1` matrix into a n-by-1 matrix, in which `n` is the total number of values in the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7497c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xg = Xg1.reshape(((1, -1)), order = \"F\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fbc36",
   "metadata": {},
   "source": [
    "We can do the same for the y coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a90b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yg = Xg2.reshape(((1, -1)), order = \"F\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ccaa9",
   "metadata": {},
   "source": [
    "Now, we will just create some empty (NaN) matrices in which we will store the kriging outputs of the next step. This will be the matrix that will later contain our kriging estimates of `z`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffd3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zg = Xg * np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54434f",
   "metadata": {},
   "source": [
    "This will be the matrix in which we store the kriging variance, which we can use to assess the reliability of our result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_k = Xg * np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4186547",
   "metadata": {},
   "source": [
    "Now that everything is in place, we can start with our kriging. This is where we will use some slightly more advanced Python code, so pay close attention to the comments in the code below to make sure you understand what everything does.\n",
    "\n",
    "In short, we will use a **for-loop** to loop through all the new datapoints and apply the kriging algorithm to estimate the `z` value for each point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f090bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(Xg)): # Loop through all the values in Xg\n",
    "    DOR = ((x - Xg[k]) ** 2 + (y - Yg[k]) ** 2) ** 0.5 # This is the distance between the grid points (Xg(k),Yg(k)) and the observed points (x, y) using Pythagorean theorem\n",
    "    G_R = (nuggetL + sillE * (1 - np.exp(-3 * DOR / rangeE))) * (DOR > 0) # We now calculate the semivariance estimated from the variogram for all these distances.\n",
    "    G_R = np.vstack((G_R, np.ones((1, 1)))) # We add a 1 to the semivariance matrix G_R\n",
    "    E = np.dot(G_inv, G_R) # To derive the weights E for the kriging operation, we multiply the semivariance vector G_R with G_inv\n",
    "    Zg[k] = np.sum(E[0:n, 0] * z[:, 0]) # Now we can calculate the z-value of the grid cell as the weighted sum of the observations, E being the weights.\n",
    "    s2_k[k] = np.sum(E[0:n, 0] * G_R[0:n, 0]) + E[n, 0] # We can also calculate the kriging variance as the weighted sum of the semivariance matrix G_R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb590dd7",
   "metadata": {},
   "source": [
    "## Making sense of our kriging result\n",
    "\n",
    "OK, that was quite a lot to take in. It's OK if you do not understand all the knitty-gritty detail of the calculations above. What is important is that you see that the algorithm produces estimates of the z values (in vector `Zg`) and estimates of the variance around these values (which can be interpreted as model uncertainty, in `s2_k`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a385c525",
   "metadata": {},
   "source": [
    "We now have all our results stored in vectors (with dimentions n by 1). To visualize our kriging result as a map, we want to go back from vectors to matrices (grids) and we can use the `reshape` function again to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4357b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = len(R) # This is the number of rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc396aa3",
   "metadata": {},
   "source": [
    "The `reshape` function outputs a grid matrix with r rows/columns from the `Zg` vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.reshape(Zg, (r, r) , order = \"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9002d",
   "metadata": {},
   "source": [
    "And we can do the same for the kriging variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SK = np.reshape(s2_k, (r, r) , order = \"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08909989",
   "metadata": {},
   "source": [
    "Now, finally, we can plot our result as a map with x and y coordinates on the horizontal and vertical. We call such a plot where the values in a matrix are represented by a color scale a **pseudocolor plot**. We will use the function `pcolor` (`matplotlob` package) for this. Let's first have a look at the `help` for this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c304a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(plt.pcolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c96083",
   "metadata": {},
   "source": [
    "The code below walks you through the plot step by step. Make sure you understand what's going on by reading the comments next to each line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(121) # This creates enough space for 2 figures next to each other\n",
    "plt.pcolor(Xg1, Xg2, Z) # Create the pseudocolor plot (map) with Xg1 and Xg2 providing the coordinates and Z containing the values to be plotted as colors.\n",
    "plt.gca().set_aspect('equal', adjustable='box') # This ensures equal distances for the x and y grid cell intervals in the visualization\n",
    "plt.ylim((0, 195)) # But now, we need to adjust the limits of y axis so only existing grid cells are displayed\n",
    "plt.title('Kriging estimate')\n",
    "plt.xlabel('x coordinates')\n",
    "plt.ylabel('y coordinates')\n",
    "plt.colorbar()# This adds a legend for the displayed colors\n",
    "\n",
    "# We will use the same commands to plot the kriging variance next to it.\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.pcolor(Xg1, Xg2, SK) # SK is the estimated kriging variance\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.ylim((0, 195))\n",
    "plt.title('Kriging variance')\n",
    "plt.xlabel('x coordinates')\n",
    "plt.ylabel('y coordinates')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638c899",
   "metadata": {},
   "source": [
    "Finally, we can plot the original observations on top of the variance map using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d20493",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(Xg1, Xg2, SK) # SK is the estimated kriging variance\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.ylim((0, 195))\n",
    "plt.title('Kriging variance')\n",
    "plt.xlabel('x coordinates')\n",
    "plt.ylabel('y coordinates')\n",
    "plt.colorbar()\n",
    "plt.scatter(x, y, s = 80, facecolors = 'none', edgecolors = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefcb3a",
   "metadata": {},
   "source": [
    "**Question 6:** What do you observe regarding the spatial pattern of the kriging variance in relation to the observed point locations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a8eee",
   "metadata": {},
   "source": [
    "**Answer 6:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
