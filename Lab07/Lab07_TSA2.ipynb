{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d8531b",
   "metadata": {},
   "source": [
    "# Lab 07: Time series analysis 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da6f20",
   "metadata": {},
   "source": [
    "Author: **N.J. de Winter** (*n.j.de.winter@vu.nl*)<br>\n",
    "Assitant Professor Vrije Universiteit Amsterdam<br>\n",
    "Statistics and Data Analysis Course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92b3df",
   "metadata": {},
   "source": [
    "\n",
    "## Learning goals:\n",
    "\n",
    "* Apply and improve your knowledge of Python and Jupyter\n",
    "* Learn to apply Fourier analysis to extract dominant frequencies from time series and interpret them\n",
    "* Learn to fit frequency distributions to time series data and interpret the results to quantify the chances of extreme events to occur\n",
    "* Develop a feeling for how statistical tools can help you, but you still require *your interpretation* to draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a47a00",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this lab, we will continue to experiment with **time series analysis**. Last lab, we have focused on the occurrence of periodic events and changes. This time, we will take our data analysis one step further by looking at the frequency and probability of events that do not occur at a regular interval. We call these events **extreme events** and they play an important role in society."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7364d",
   "metadata": {},
   "source": [
    "**Exercise 1:** You know the drill by now: Let's load some packages! For this lab, we need the familiar `numpy` and `matplotlib.pyplot` packages as well as the `signal` package from `scipy` (see **Lab06**), the `loadmat` package from `scipy.io` (see **Lab05**) and the `pandas` package (see **Lab02**). Load them using the code box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27764eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d888173",
   "metadata": {},
   "source": [
    "**Exercise 2:** Now use the `loadmat` function to load the data file `lab07.mat` (Tip: you've done this before in **Lab05**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f81f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f78bf4a",
   "metadata": {},
   "source": [
    "The loadmat function returns a dictionary with variables stored as keys. We can output all keys, and then access the data of each variable using the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cae8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys() # Have a look at the keys of the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e06d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data['X'] # Extract the data belonging to key `X`\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546fa23c",
   "metadata": {},
   "source": [
    "In this dataset, the key `t` indicates the time vector and the key `data` contains the values of our dependent variable. Together they make up our time series. We can parse the time and data vectors in 1 matrix to make it easier to work with them. Study the code below to understand what it does and then inspect the end result using a `print()` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecb5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.concatenate((data['t'], data['data']), axis=1), columns=['t', 'data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4ecc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fd0058c",
   "metadata": {},
   "source": [
    "The **lab07** file contains data on daily drainage of the Dinkel river in Twente (in $m^3/s$). The data covers 12 years, from 1988 until 1999. The variable `data` contains the drainage values. The variable `t` contains the day number since January 1, 1988. The variable `X` will be used later in the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8e834",
   "metadata": {},
   "source": [
    "**Exercise 3:** Explore the data by making a time series plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f52886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2612f1c",
   "metadata": {},
   "source": [
    "**Exercise 4:** We are going to do the frequency analysis of the Dinkel river discharge. Let's use the function `signal.periodogram` that we used in **Lab07** for this. Use the code box below to do this frequency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03c054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abdfd87e",
   "metadata": {},
   "source": [
    "You were probably playing around with the parameters of the `signal.periodogram` function quite a lot to get the function to work. Luckily, we can make our life easier by leaving all those parameters empty. The function then just picks default values for `window`, `fs` and `nfft`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cdcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, Pxx = signal.periodogram(df.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d2e3c",
   "metadata": {},
   "source": [
    "Make sure that you understand the output of the signal.periodogram function well. If necessary, have a look at **Lab06** or the help function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c918b98",
   "metadata": {},
   "source": [
    "**Exercise 5:** Plot the output as a periodogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75382e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7497bffd",
   "metadata": {},
   "source": [
    "**Exercise 6:** Now zoom in a bit on the left side of the graph. You can do this either by only plotting the first 100 values of the input `f` and `Pxx` in the `plt.plot` statement (e.g. `f[1:100]`) or by changing the x-axis limits (see **Lab06**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6949f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "836d8abf",
   "metadata": {},
   "source": [
    "**Question 1:** For which frequency does the variance peak? What is the corresponding wavelength and what does this represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028f7ff",
   "metadata": {},
   "source": [
    "**Answer 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e13a3",
   "metadata": {},
   "source": [
    "The variable `X` contains middle values of different classes to construct a histogram of daily discharges. The values in `X` range between 0.5 and 34.5. Classes are spaced with intervals of 1 $m^3/s$. We will use the `matplotlib` function `hist` to plot a *histogram*. First, we have to calculate the bins (interval breaks) from `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6383ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.unique(np.concatenate((X - 0.5, X + 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e343e87",
   "metadata": {},
   "source": [
    "Now we plot the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bcbc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df.data, bins = bins)\n",
    "plt.xlabel('Daily discharge (m3/s)')\n",
    "plt.ylabel('Number of days')\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94402e0a",
   "metadata": {},
   "source": [
    "**Question 2:** What do you think about the number of low and and high discharge days? Is the pattern in the histogram as you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f04a8",
   "metadata": {},
   "source": [
    "**Answer 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a8f18",
   "metadata": {},
   "source": [
    "Now, we will calculate frequencies of probability dividing the histogram by the total number of days. We first use the function `histogram` (`numpy`) to calculate the histogram statistics, and then divided it by the total number of days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a361d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = np.histogram(df.data, bins = bins)\n",
    "N = hist_data[0] / float(len(df.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517009ef",
   "metadata": {},
   "source": [
    "**Exercise 7:** Use a graph to check your result from the calculations above. You can use the plotting techniques (e.g. `plt.plot`) to do this, but you have to transform your `X` values to the right format. You can do so by using the `np.concatenate` function you used in the calculation of the `bins` vector above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e479d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "410de0fc",
   "metadata": {},
   "source": [
    "We will fit the *Gumbel distribution* to our data. The Gumbel function is often used for flood analysis because its shape allows for outliers in the data. The Gumbel distribution is defined by the parameters *alfa* and *mu*, which determine the shape of the distribution. *Mu* determines for which values the distribution peaks, while *alfa* determines the amplitude of the peak. *Mu* and *alfa* can be calculated from the mean and standard deviation of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6736824",
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa = 0.7797 * np.std(df.data)\n",
    "mu = np.mean(df.data) - 0.5772 * alfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbdaaee",
   "metadata": {},
   "source": [
    "Study the code above to make sure you understand what is going on. In the process, you have learned a useful function for calculating the *mean* and *standard deviation* of a dataset automatically in Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dcfdd2",
   "metadata": {},
   "source": [
    "Next, we can use the Gumbel function to calculate the theoretical probabilities of different discharges. To do this, we will compute the cumulative probabilities for different discharges for the boundaries of the bins and then subtract those from each other to get the probabilities for discharges within the bins. We will start by calculating cumulative probabilities twice, in which the second array represent the same bin boundaries but with 1 added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c553a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins2 = bins + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579f70de",
   "metadata": {},
   "source": [
    "The cumulative probabilities now follow after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = np.exp(-np.exp(-(bins - mu) / alfa))\n",
    "p2 = np.exp(-np.exp(-(bins2 - mu) / alfa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc7c31",
   "metadata": {},
   "source": [
    "Now we can calculate the modelled probability per bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbbcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p2 - p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b8d62",
   "metadata": {},
   "source": [
    "Let's now plot the histogram of our data and the Gumbel distribution fit to our data in one graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(np.arange(0.5, 35.5, 1), N) # Plot the histogram\n",
    "plt.plot(np.arange(0.5, 36.5, 1), p, color = 'r') # Plot the modelled Gumbel distribution\n",
    "plt.xlabel('Daily discharge (m3/s)')\n",
    "plt.ylabel('Frequency/Probability')\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da92b5b",
   "metadata": {},
   "source": [
    "**Question 3:** What do you think about the fit of our Gumbel distribution model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0084b2e",
   "metadata": {},
   "source": [
    "**Answer 3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea8393",
   "metadata": {},
   "source": [
    "Letâ€™s calculate the *recurrence time* of a discharge larger than 25 $m^3$ based on the Gumbel distribution. We will therefore first calculate what the probability of a discharge lower than 25 $m^3$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28036ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lowerthan25 = np.exp(-np.exp(-(25 - mu) / alfa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19a899",
   "metadata": {},
   "source": [
    "**Exercise 8:** Sicne we know that probabilities always have a total sum of 1 (or 100% chance), you can easily figure out what the probability of a discharge higher than 25 (`p_higherthan25`) then must be. Calculate it in the box below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32add20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6882e2a9",
   "metadata": {},
   "source": [
    "**Exercise 9:** Knowing that the *recurrence time* of an event is the inverse of its probability, calculate the recurrence time of a discharge higher than 25 $m^3/s$ (`T_higherthan25`) in the box below and inspect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411e10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccffe3fc",
   "metadata": {},
   "source": [
    "To check whether our model (the Gumbel distribution) of the frequency of flood events in the Dinkel river gives us a good prediction of the recurrence interval, we can compare this result with the observed frequency of discharges larger than 25 $m^3/s$ by counting the number of values larger than 25 $m^3/s$ in the data. Use the len and np.where functions to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dischargeover25 = np.size(np.where(df.data > 25)) # Count the number of occurrences with a discharge over 25 m3/s in the data\n",
    "p_Dischargeover25 = Dischargeover25 / float(len(df.data)) # Divide by the total number of days in the dataset to obtain the probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410d2c6",
   "metadata": {},
   "source": [
    "**Exercise 10:** From this probability, now calculate the *observed* recurrence time (`T_higherthan25_obs`) in the same way as you calculated `T_higherthan25` in **Exercise 9** and inspect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d28e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1371c26b",
   "metadata": {},
   "source": [
    "**Question 4:** Now compare the recurrence time of discharge larger than 25 $m^3$ estimated from the Gumbel distribution (`T_higherthan25`) with the observed recurrence time (`T_higherthan25_obs`). How do they compare and why do you think this is the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee77c8e",
   "metadata": {},
   "source": [
    "**Answer 4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078c95f",
   "metadata": {},
   "source": [
    "**Question 5:** It turns out that low frequency events (with a high recurrence interval) are almost always harder to predict than events that occur more frequenty. Why do you think this is the case? Can you think of a solution to make a better model for the discharge of the Dinkel river and for these type of event in general? What is the most important thing we can do to better understand these distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055e1e4",
   "metadata": {},
   "source": [
    "**Answer 5:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
